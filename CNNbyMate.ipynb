{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take a significant amount of time!\n",
    "# Print a list of single channel coin images and other images\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "coin_path = r'C:\\Users\\Mate\\Projects\\Coin\\data\\coin_256x256' # Replace with path to coin images\n",
    "others_path = r'C:\\Users\\Mate\\Projects\\Coin\\data\\others_256x256' # Replace with path to other images\n",
    "\n",
    "def list_monochromes(input_path):\n",
    "    file_list = os.listdir(input_path)\n",
    "    monochrome_list = []\n",
    "\n",
    "    for item in file_list:\n",
    "        if item != 'desktop.ini':\n",
    "            current_image = Image.open(input_path + '\\\\' + item)\n",
    "            if len(current_image.getbands()) == 1:\n",
    "                monochrome_list.append(item)\n",
    "            current_image.close()    \n",
    "\n",
    "    return monochrome_list\n",
    "\n",
    "print(list_monochromes(coin_path))\n",
    "print(list_monochromes(others_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only use 10k images from coin and other each (20k total)\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths to the subfolders\n",
    "folder_path1 = r'C:\\Users\\Mate\\Projects\\Coin\\data\\coin_256x256'\n",
    "folder_path2 = r'C:\\Users\\Mate\\Projects\\Coin\\data\\others_256x256'\n",
    "\n",
    "# Function to load a subset of images from a folder\n",
    "def load_subset_images(folder_path, num_images):\n",
    "    file_list = os.listdir(folder_path)\n",
    "    # Ensure you have a maximum of num_images\n",
    "    num_images = min(num_images, len(file_list))\n",
    "    \n",
    "    # Randomly select num_images from the list of files\n",
    "    selected_files = random.sample(file_list, num_images)\n",
    "    \n",
    "    # Create a list of (image_path, label) pairs\n",
    "    image_paths = [os.path.join(folder_path, file) for file in selected_files]\n",
    "    \n",
    "    # Label for this subset can be determined based on the folder it came from\n",
    "    label = 1 if \"coin_\" in folder_path else 0\n",
    "    \n",
    "    # Create a list of (image_path, label) pairs\n",
    "    dataset = [(image_path, label) for image_path in image_paths]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load subsets of images from the two subfolders\n",
    "num_images_per_folder = 10000\n",
    "subset1 = load_subset_images(folder_path1, num_images_per_folder)\n",
    "subset2 = load_subset_images(folder_path2, num_images_per_folder)\n",
    "\n",
    "# Combine the subsets\n",
    "combined_dataset = subset1 + subset2\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.dataset[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transform the data \n",
    "transforms = transforms.Compose([\n",
    "    # transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) -> Investigate!\n",
    "])\n",
    "\n",
    "# Create a custom dataset\n",
    "custom_dataset = CustomDataset(combined_dataset, transform=transforms)\n",
    "\n",
    "# Define dataset sizes\n",
    "train_size = int(0.7 * len(custom_dataset))\n",
    "val_size = int(0.15 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size - val_size\n",
    "\n",
    "# Split the data\n",
    "train_dataset, val_dataset, test_dataset = random_split(custom_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders for the splits\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch CNN: \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Docs: \n",
    "# https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model \n",
    "# https://medium.com/thecyphy/train-cnn-model-with-pytorch-21dafb918f48\n",
    "\n",
    "class CoinClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CoinClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(12) # -> Investigate!\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(24)\n",
    "        #self.fc1 = nn.Linear(24*10*10, 10)\n",
    "        \n",
    "        # Calculate the size of the first linear layer based on the input size\n",
    "        self.first_linear_size = self.calculate_first_linear_size(input_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.first_linear_size, 1)\n",
    "\n",
    "    def calculate_first_linear_size(self, input_size):\n",
    "        # Compute the output size after passing through the convolutional and pooling layers\n",
    "        conv1_output = ((input_size - 3 + 2 * 1) // 2) + 1\n",
    "        conv2_output = ((conv1_output - 5 + 2 * 1) + 1)\n",
    "        pool_output = conv2_output // 2\n",
    "        conv4_output = ((pool_output - 5 + 2 * 1) + 1)\n",
    "        conv5_output = ((conv4_output - 5 + 2 * 1) + 1)\n",
    "\n",
    "        # Calculate the size of the flattened vector for the linear layer\n",
    "        first_linear_size = 24 * conv5_output * conv5_output\n",
    "\n",
    "        return first_linear_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool(output)                        \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = F.relu(self.bn5(self.conv5(output)))     \n",
    "        #output = output.view(-1, 24*10*10)\n",
    "        output = output.view(-1, self.first_linear_size)\n",
    "        output = self.fc1(output)\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Define the input size based on your data\n",
    "input_size = 256\n",
    "\n",
    "model = CoinClassifier(input_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer algorithm\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Binary Cross Entropy - https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html \n",
    "loss_fn = nn.BCELoss(reduction='mean')\n",
    "# Adam - Gradient Descent\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001) # -> Investigate the lr and weight-decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [1, 256, 256] at entry 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\Classification-and-3D-reconstruction-of-archaeological-artifacts\\CNNbyMate.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mate/Projects/Coin/Classification-and-3D-reconstruction-of-archaeological-artifacts/CNNbyMate.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mate/Projects/Coin/Classification-and-3D-reconstruction-of-archaeological-artifacts/CNNbyMate.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mate/Projects/Coin/Classification-and-3D-reconstruction-of-archaeological-artifacts/CNNbyMate.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data, labels \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mate/Projects/Coin/Classification-and-3D-reconstruction-of-archaeological-artifacts/CNNbyMate.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mate/Projects/Coin/Classification-and-3D-reconstruction-of-archaeological-artifacts/CNNbyMate.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#Collect and calculate metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Mate\\Projects\\Coin\\coinvenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [1, 256, 256] at entry 8"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        labels = labels.view(-1, 1).float()\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#Validation Loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, labels in val_loader:\n",
    "        outputs = model(data)\n",
    "model.train()\n",
    "\n",
    "#Testing Loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data)\n",
    "\n",
    "\n",
    "#Collect and calculate metrics\n",
    "true_labels = []  # List to store true labels\n",
    "predicted_probs = []  # List to store predicted probabilities\n",
    "predicted_labels = []  # List to store predicted labels\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data)\n",
    "        true_labels.extend(labels.numpy())\n",
    "        predicted_probs.extend(torch.sigmoid(outputs).numpy())\n",
    "        predicted_labels.extend((outputs >= 0.5).numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_probs = np.array(predicted_probs)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "roc_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, _ = roc_curve(true_labels, predicted_probs)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coinvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
