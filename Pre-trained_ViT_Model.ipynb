{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eeb2e10-ad43-41f4-ad22-3a3c2062f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/main/models/vision_transformer.html\n",
    "# https://huggingface.co/blog/fine-tune-vit\n",
    "# https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "# ViT model: https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py#L37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492e8a9-6ff7-464c-943c-5e5181bf56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from external source\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "# Docs:\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?highlight=dataloader\n",
    "\n",
    "# 256x256 images\n",
    "path_to_coins = \"/run/media/magnusjsc/T7/Classification-and-3D-reconstruction-of-archaeological-artifacts_DATA/resized_images_coin_256x256/\"\n",
    "path_to_others = \"/run/media/magnusjsc/T7/Classification-and-3D-reconstruction-of-archaeological-artifacts_DATA/resized_images_others_256x256/\"\n",
    "\n",
    "'''\n",
    "path_to_coins = \"/mnt/c/Users/bucha/Pictures/resized_images_coin_256x256\"\n",
    "path_to_others = \"/mnt/c/Users/bucha/Pictures/resized_images_others_256x256\"\n",
    "'''\n",
    "\n",
    "def load_data_from_directory_with_limit(directory_path, label, limit=10000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".jpg\"): \n",
    "            image_path = os.path.join(directory_path, filename)\n",
    "            image = read_image(image_path)\n",
    "\n",
    "            # Check if the image is RGB\n",
    "            if image.shape[0] == 3:\n",
    "                data.append(image)\n",
    "                labels.append(label)\n",
    "                count += 1\n",
    "\n",
    "            if count >= limit:\n",
    "                break\n",
    "\n",
    "    return data, labels # Tuple\n",
    "\n",
    "# Load data call\n",
    "data_coins, labels_coins = load_data_from_directory_with_limit(path_to_coins, label=1, limit=100)\n",
    "data_others, labels_others = load_data_from_directory_with_limit(path_to_others, label=0, limit=100)\n",
    "\n",
    "print(f'Size of data_coins {len(data_coins)} : Size of labels_coins {len(labels_coins)}')\n",
    "print(f'Size of data_others {len(data_others)} : Size of labels_others {len(labels_others)}')\n",
    "\n",
    "def imshow_grid(images, num_images = 30):\n",
    "    grid = torchvision.utils.make_grid(images[:num_images], nrow = 10)\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    plt.title('Sample Images')\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "num_images_to_display_per_category = 15\n",
    "data_to_display = data_coins[:num_images_to_display_per_category] + data_others[:num_images_to_display_per_category]\n",
    "imshow_grid(data_to_display, num_images=num_images_to_display_per_category * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af779cdb-647c-4c12-9c3e-b9e29eff4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        # Check the sizes of data and labels match\n",
    "        assert all(tensors[0].size(0)==t.size(0) for t in tensors), \"Size mismatch between tensors\"\n",
    "\n",
    "        # Assign input\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    # What to do when we want an item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        # Return the transformed version of x if there are transforms\n",
    "        if self.transform:\n",
    "            x = self.transform(self.tensors[0][index])\n",
    "        else: \n",
    "            x = self.tensors[0][index]\n",
    "\n",
    "        # And return the label too\n",
    "        y = self.tensors[1][index].view(1) # Convert y to a single-element tensor\n",
    "\n",
    "        return x,y # Return the data as a tuple (data,label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "\n",
    "# Create a list of  transforms\n",
    "imgTrans = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(), # PIL = library for images - Works for several transformations. To PIL image type\n",
    "        transforms.RandomVerticalFlip(p=.5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor() # Transform the image to PyTorch tensor and normalizes the data [0;1] - Last \n",
    "    ]\n",
    ")\n",
    "\n",
    "# To Torch tensors\n",
    "coin_dataT = torch.stack(data_coins)\n",
    "coin_labelsT = torch.tensor(labels_coins)\n",
    "other_dataT = torch.stack(data_others)\n",
    "other_labelsT = torch.tensor(labels_others)\n",
    "\n",
    "# For the Coin dataset\n",
    "coin_data_transformed_defined = customDataset(tensors=(coin_dataT, coin_labelsT), transform=imgTrans)\n",
    "# For the Other dataset\n",
    "other_data_transformed_defined = customDataset(tensors=(other_dataT, other_labelsT), transform=imgTrans)\n",
    "# Combine these two above datasets\n",
    "combined_dataset_transformed_defined = ConcatDataset([coin_data_transformed_defined, other_data_transformed_defined])\n",
    "\n",
    "# Split the data for training, validation, and testing\n",
    "training = 0.7\n",
    "validation = 0.15\n",
    "testing = 0.15\n",
    "\n",
    "total_size = len(combined_dataset_transformed_defined)\n",
    "train_size = int(training * total_size)\n",
    "val_size = int(validation * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Randomly split the full dataset into training, validation and testing sets\n",
    "train_set, val_set, test_set = random_split(combined_dataset_transformed_defined, [train_size, val_size, test_size])\n",
    "\n",
    "# Hyperparameter - Batch size - GPU may be overloaded with memory allocations if too high\n",
    "BATCH_SIZE = 50\n",
    "# Create data loaders for each set\n",
    "coin_loader = DataLoader(coin_data_transformed_defined, batch_size=BATCH_SIZE, shuffle=False) # For test -> below cell\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) # Shuffle the training set\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f'Total number of elements in train_loader: {len(train_loader) * BATCH_SIZE} : Each batch size {len(train_loader)}')\n",
    "print(f'Total number of elements in val_loader: {len(val_loader) * BATCH_SIZE} : Each batch size {len(val_loader)}')\n",
    "print(f'Total number of elements in test_loader: {len(test_loader) * BATCH_SIZE} : Each batch size {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56728f58-76b2-4dc0-8ac4-bae04d4396a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, AdamW\n",
    "import torch\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "vit_model = ViTForImageClassification.from_pretrained(model_name_or_path)\n",
    "# processor.size = {\"height\": 256, \"width\": 256}\n",
    "\n",
    "# Model configs\n",
    "print(vit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e78c237-7d3c-41dc-877b-f577b13e8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magnusjsc/repos/Classification-and-3D-reconstruction-of-archaeological-artifacts/linuxPythonVenv/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     10\u001b[0m     vit_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m     12\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "optimizer = AdamW(vit_model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vit_model.to(device)\n",
    "\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"\n",
    "    Compute accuracy for binary classification.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Model predictions.\n",
    "        labels (torch.Tensor): Ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy value.\n",
    "    \"\"\"\n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = torch.sigmoid(predictions)\n",
    "\n",
    "    # Convert probabilities to binary predictions (0 or 1)\n",
    "    binary_predictions = (probabilities > 0.5).float()\n",
    "\n",
    "    # Compare binary predictions with ground truth labels\n",
    "    correct_predictions = (binary_predictions == labels).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions.mean().item()\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit_model(inputs)\n",
    "        loss = criterion(outputs.logits, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "     \n",
    "            outputs = vit_model(inputs)\n",
    "            val_loss = criterion(outputs.logits, labels.float())\n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {average_val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fcf219-cdcc-4c43-870a-d1ccf0e56135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
